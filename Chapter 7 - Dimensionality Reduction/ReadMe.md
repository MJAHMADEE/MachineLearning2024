# Dimensionality Reduction Techniques

Welcome to our repository dedicated to exploring Dimensionality Reduction techniques for educational purposes in our Machine Learning course. This repository aims to provide comprehensive resources for understanding Dimensionality Reduction through practical examples and custom implementations.

## Resources

Access additional resources including videos, codes, and more via our Google Drive:

[![Google Drive](https://img.shields.io/badge/Google%20Drive-Access%20Resources-blue?style=for-the-badge&logo=google-drive)](https://drive.google.com/drive/folders/1qc5DrD14rh1JHKqFUjJ71duQwzOqfY4U?usp=sharing)


## Overview

In this repository, we offer implementations of various Dimensionality Reduction techniques:

1. **Principal Component Analysis (PCA)**: Understand and implement PCA, a popular linear dimensionality reduction technique, to reduce the number of features while preserving most of the variance in the data.
   
2. **Linear Discriminant Analysis (LDA)**: Explore LDA, a supervised dimensionality reduction technique, which aims to maximize the separation between different classes while reducing dimensionality.
   
3. **t-Distributed Stochastic Neighbor Embedding (t-SNE)**: Dive into t-SNE, a non-linear dimensionality reduction technique particularly useful for visualizing high-dimensional data in lower dimensions while preserving local structure.

## Features

We delve into various scenarios and advanced topics in Dimensionality Reduction, including:

- **Visualization**: Understand how dimensionality reduction techniques can aid in visualizing high-dimensional data in two or three dimensions.
  
- **Feature Engineering**: Learn how to use dimensionality reduction as a tool for feature engineering, which can improve the performance of machine learning models.

# Dimensionality Reduction Implementations and Demonstrations

Explore our collection of implementations and demonstrations showcasing Dimensionality Reduction techniques, highlighting diverse methods and their applications.

## Content

### 1. PCA (from Scratch)
- An implementation of Principal Component Analysis built from scratch using NumPy.

### 2. LDA (from Scratch)
- Construct a simplified version of Linear Discriminant Analysis without relying on existing libraries.

### 3. t-SNE (from Scratch)
- Implement t-SNE algorithm from scratch to understand its inner workings.

### 4. Dimensionality Reduction (Scikit-learn)
- Demonstrations of Dimensionality Reduction techniques utilizing Scikit-learn's functionalities for various tasks.

---
Feel free to explore and learn more about Dimensionality Reduction techniques through our repository!
